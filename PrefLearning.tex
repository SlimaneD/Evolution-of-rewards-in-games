\documentclass[11pt,reqno]{amsart}
\usepackage[parfill]{parskip}
\usepackage{graphicx}

\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{sgame}

\usepackage[dvipsnames,table]{xcolor}
\usepackage{colortbl}

\usepackage{amssymb}
\usepackage{bbm}
%\usepackage[urw-garamond]{mathdesign}
%\usepackage{nomencl}
%\usepackage[type1]{libertine}
%\usepackage[T1]{fontenc}
%\usepackage[libertine]{newtxmath}
\usepackage{dsfont}
%\usepackage{classico}
%\usepackage{merriweather}

\usepackage[T1]{fontenc}
\usepackage{stix}
\usepackage{rotating}
\usepackage{booktabs}


\usepackage[margin=0.8in]{geometry}

%%% PAGE DIMENSIONS
%\usepackage{geometry} % to change the page dimensions
%\geometry{a4paper} % or letterpaper (US) or a5paper or....
%\geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent


%\DeclareSymbolFont{cmlargesymbols}{OMX}{cmex}{m}{n}
%\let\sum\relax
%\DeclareMathSymbol{\sum}{\mathop}{cmlargesymbols}{"50}
%
%\DeclareSymbolFont{cmlargesymbols}{OMX}{cmex}{m}{n}
%\let\int\relax
%\DeclareMathSymbol{\int}{\mathop}{cmlargesymbols}{"52}


\newtheorem{theorem}{\textsc{Theorem}}
\newtheorem{definition}{\textsc{Definition}}
\newtheorem{proposition}{\textsc{Proposition}}

\renewcommand{\baselinestretch}{1.2}
%\setlength{\textwidth}{15cm}


\newcommand{\st}{\rho}
\newcommand{\ha}{h}
\newcommand{\en}{e}
\newcommand{\En}{E}
\newcommand{\e}{\theta}
\newcommand{\Et}{\Theta}
\newcommand{\At}{V}
\newcommand{\dAt}{\Delta\At}
\newcommand{\na}{n}
\newcommand{\ac}{a}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\p}{p}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\de}{\mathop{}\!\mathrm{d}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vm}{\mathbf{\At}}
\newcommand{\nba}{n}
\newcommand{\iv}{\mathds{1}}
\newcommand{\Rp}{\mathcal{R}}
\newcommand{\Sp}{\mathcal{S}}
\newcommand{\Tp}{\mathcal{T}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\pay}{\pi}
\newcommand{\lr}{\gamma}
\newcommand{\xr}{\xi}


\newcommand{\Esp}{\mathds{E}}
\newcommand{\Prb}{\mathds{P}}
\newcommand{\Rn}{\mathds{R}}


\newcommand{\np}{N}%number of players
\newcommand{\tm}{t}%time index
\newcommand{\om}{o}
\newcommand{\Om}{O}
\newcommand{\fc}{f}
\newcommand{\ru}{\ell}
\newcommand{\pf}{u}
\newcommand{\vpf}{\mathbf{u}}
\newcommand{\Pf}{U}
\newcommand{\ma}{\mu}
\newcommand{\sd}{\sigma}
\newcommand{\Nd}{\mathcal{N}}
\newcommand{\vma}{\boldsymbol{\ma}}
\newcommand{\rf}{R}
\newcommand{\vrf}{\mathbf{\rf}}
\newcommand{\pd}{\partial}
\newcommand{\Hs}{\mathbf{H}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\mx}{M}
\newcommand{\dv}{\delta}
\newcommand{\vd}{\boldsymbol{\dv}}
\newcommand{\ba}{\beta}
\newcommand{\bn}{b}
\newcommand{\cs}{c}
\newcommand{\vh}{k}

%\title[Preferences and learning]{Evolution of rewards in games}
\title{The evolutionary origin of social rewards}
\date{\today}

\begin{document}
\maketitle







\section{Model}


\subsection{Game, payoffs, and utilities}


We consider a general $\np$-player game where each player $i$ has an action set $\Ac_i$ from which it can choose an action $\ac_i \in \Ac_i$. We think of these $\np$ players as belonging to a larger population, where a (random) matching has occurred between population members that generated many groups of $\np$ players (see below the evolutionary setting). The action space $\Ac_i$ can be a finite set of $\na$ discrete actions or a continuous set, such as a subset of $\Rn$. The environment as well as the actions of the other players affect the material payoff of every individual. There is a set $\Et$ of environmental states, with typical element $\e\in\Et$ (the environment can also be continuous or discrete). The material payoff of a player is given by the function $\pay_i : \prod_{j\in \np} \Ac_j \times \Et  \to \Rn$, which ultimately affects the fitness of $i$ (see below for details). To simplify notation, we will also write $\Ac = \prod_{j\in \np} \Ac_j$ for the set of all possible action profiles.

The players have subjective preference or utility functions, $\pf_i : \prod_{j\in \np} \Ac_j \times \Et  \to \Rn$, that may differ from the objective material payoff $\pay_i$. The utility function $\pf_i$ is genetically determined and is the evolving trait: we are interested in the function(s) $\pf_i$ that is(are) favored by natural selection. In the forthcoming analysis of the model, it may be easier to think of the game as having a single-valued ``outcome'', $\om$ that is a function of the action profile and the environment, i.e.~$\om(\va,\e)\in \Om$, where the outcome space, $\Om$, is a subset of $\Rn$. The utility function may then be defined directly over outcomes $\pf_i : \Om  \to \Rn$. For example, in a public-goods game, the outcome may be defined as the group productivity (e.g.~the sum of all players' contributions multiplied by a synergy factor). The reason we make these additional definitions about game outcomes is that it may be easier to find general evolutionarily stable utility functions that are univariate (when the outcome is single-valued) than multivariate (when the outcome is the combination of the action profile and the state of the environment), and we could use the techniques of Kirkpatrick \& Heckman (1989) to perform such an analysis.

The game is repeated at discrete times $\tm = 1,2,\dots$ and the players choose an action profile $\va_\tm$. The environment fluctuates independently from the players' actions and takes value $\e_\tm$ at time $\tm$. The probability that state $\e$ occurs at time $\tm$ is written $\st(\e)$, and is thus independent of time (i.i.d. environment). The players cannot observe the state of the environment prior to choosing their actions. The material payoff to player $i$ when the action profile is $\va_\tm$ and the environment is in state $\e_\tm$ is given by $\pay_i(\va_\tm,\e_\tm)$, which we may also write $\pay_i(\ac_{i,t}, \va_{-i,t},\e_\tm)$, where $\va_{-i,t} \in \prod_{\substack{j\in \np \\ j\neq i}} \Ac_j$ is the action profile of all players except player $i$. Each individual observes privately the utility $\pf_i(\va_\tm,\e_\tm)$ which results from the game outcome $\om_\tm = (\va_\tm,\e_\tm)$ at time $\tm$.



\subsection{Learning}

The learning model is taken from Dridi \& Lehmann (2014), where players use the material payoffs of the game to update motivations about actions. Our learning model will not be very different from this previous work because all we have to change is that individuals update their choice probabilities using the subjective utility of a game outcome, rather than the objective material payoff. Hence, the difference between this previous paper and our formulation of learning dynamics is that in Dridi \& Lehmann (2014), the games considered are symmetric, while here the utility function of each player is different, which translates into an asymmetric game defined by the family of utility functions $(\pf_i)_{i\in \np}$.

While the general model of Dridi \& Lehmann (2014) allows for both trial-and-error learning and belief-based learning, we will first only consider trial-and-error learning. At every time $\tm$ of the repeated game defined above, each individual $i$ holds in memory action values $\At_{i,\tm}(\ac_i)$ for all actions $\ac_i \in \Ac_i$. The learning rule of individual $i$ is to update action values according to
\begin{equation}
\label{Lrule1}
\At_{i,\tm+1}(\ac_i) = (1-\lr_\tm) \At_{i,\tm}(\ac_i) + \lr_\tm  \iv(\ac_i,\ac_{i,\tm}) \pf_{i}(\ac_i,\va_{-i,\tm},\e_\tm),
\end{equation} 
where $\lr_\tm \in (0,1)$ is a decreasing learning rate in the sense of stochastic approximation theory. This decreasing learning rate allows us to approximate the above stochastic difference equation with a deterministic mean-field differential equation that asymptotically tracks the original stochastic dynamics. The expression $\iv(\ac_i,\ac_{i,\tm})$ is an indicator variable that equals 1 if $\ac_i = \ac_{i,\tm}$, and 0 otherwise.

An alternative learning rule, whose mean field is an ``unperturbed''  version of the mean field of eq.~\ref{Lrule1} (which is thus easier to analyze; see Dridi \& Lehmann, 2015, for details), can be written as
\begin{equation}
\label{Lrule2}
\At_{i,\tm+1}(\ac_i) = \At_{i,\tm}(\ac_i) + \lr_\tm  \iv(\ac_i,\ac_{i,\tm}) \pf_{i}(\ac_i,\va_{-i,\tm},\e_\tm).
\end{equation} 
While eq.~\ref{Lrule2} is less used in the literature, it corresponds more to a conscious updating of action values than eq.~\ref{Lrule1} because in eq.~\ref{Lrule2} an action that is not played at time $\tm$ keeps the same action value at time $\tm+1$. Under eq.~\ref{Lrule1} action values tend to decrease for non-played actions.

We generally call the right-hand side of eqs.~\ref{Lrule1}--\ref{Lrule2} the learning rule, written $\ru_i(\At_{i}, h_i)$, of individual $i$. The learning rule takes the previous vector of action values $\At_{i}$ and a new information $h_i$ (in our present model, $h_i = \pf_{i}(\cdot,\va_{-i,\tm},\e_\tm)$, but one could think of more general updating rules) and outputs a new vector of action values.

Player $i$ chooses an action $\ac_{i,\tm}$ at time $\tm$ with a probability that depends on its action values $\At_{i,\tm} = \{ \At_{i,\tm}(\ac) \}_{\ac\in\Ac}$. One possibility is to assume a perturbed maximization scheme which gives rise to the logit-choice function,
\begin{equation}
\label{logitchoice}
\p_{i,t}(\ac_i) = \frac{\exp[\xr \At_{i,t}(\ac_i)]}{\sum_{b_i\in \Ac_i} \exp[\xr \At_{i,t}(b_i)]},
\end{equation}
where $\xr$ is the exploration parameter or noise level in choosing actions. We write $\p_{i,t}$ without the action argument to denote the whole vector of action probabilities of player $i$.

%\subsection{Reinforcement learning folk theorems}
%
%The following statements, which are valid for in the relative interior of the state space can be proved (when we use ``convergence'' below, we mean the kind of convergence that can be proved using stochastic approximation theory, i.e. convergence with probability 1) using the mean-field dynamics analysis:
%
%\begin{enumerate}[(I)]
%\item If all players have positive utility for a pure outcome, then this outcome is a locally stable equilibrium of the mean field dynamics, and has thus positive probability of being reached under the original stochastic process;
%\item If (I) is true for only one pure outcome, then this outcome is globally stable (WRONG: I FOUND A COUNTEREXAMPLE); a true statement (but trivial) is that this is the only pure outcome that is locally stable. 
%\item If (I) is false for all pure outcomes, then none of them is locally stable (this does not mean that no player will play a pure action at equilibrium);
%\item
%\end{enumerate}
%
%

\section{Fecundity}

We define the total payoff of individual $i$ as the average material payoff obtained at equilibrium of the learning process, i.e.
\begin{equation}
\label{fec}
\fc_{i} = \fc(\pf_i) = \int_{\Ac} \sum_{\e\in\Et} \hat{\vp}(\va) \st(\e) \pay_i(\va,\e) \de \va,
\end{equation}
where $\hat{\vp}(\va) = \prod_{j\in N} \hat{\p}_j(\ac_j)$ is the equilibrium probability of action profile $\va = (\ac_1,\dots,\ac_\np)$. This is the product of individuals' equilibrium action probabilities $\hat{\p}_j(\ac_j)$.

Importantly, while the utility function does not appear on the rhs of eq.~\ref{fec}, we still defined it as $\fc(\pf_i)$ because the equilibrium choice probabilities of a player, $\hat{\p}_i(\ac_i)$, will generally depend on the utility function of player $i$.

\section{Preference evolution and the set of possible utility functions}

Consider a very large population and assume that groups of $\np$ players are randomly formed at every generation to play the above repeated game. An individual's genotype corresponds to its utility function $\pf_i$, which he transmits to its offspring. The number of offspring he produces depends on its material payoffs obtained during the game as defined in eq.~\ref{fec}.

In the most general case, the set in which evolution occurs is the set of all possible utility functions $\pf : \prod_{j\in \np} \Ac_j \times \Et  \to \Rn$. Let $\Pf$ denote such a set.% We don't know if there are general techniques for studying such multivariate functions. However, there are techniques to find the univariate function that maximize a given functional. In our case, the functional is $\fc(\pf)$, and a univariate utility function $\pf(\om)$ can be defined with respect to a unidimensional game outcome $\om\in\Om$.

\section{Analysis of simple cases}

\subsection{Symmetric two-player games}

It can be shown that a trial-and-error learner's equilibrium action is such that $\pf_i(\hat{\ac}_i,\cdot) \geq 0$ (Dridi \& Lehmann, 2015). In other words, as long as the utility obtained when playing action $\hat{\ac}_i$ is positive, then $\hat{\ac}_i$ is an equilibrium action (we wrote here $\hat{\ac}_i$ for the action that has probability one under the equilibrium of the learning dynamics, i.e. $\hat{\p}_i(\hat{\ac}_i)=1$). As a consequence, there are $2^4=16$ possible types of utility functions that are relevant here because, for each of the 4 game outcomes $(\ac_i,\ac_{-i})$, the individual may have a positive or negative utility for this outcome: $\pf_i(\ac_i,\ac_{-i}) \geq 0$ or $\pf_i(\ac_i,\ac_{-i}) < 0$.

This result is nice because this means that we would not need to make any restrictive assumption on the set $\Pf$ of allowable utility functions. Statements regarding evolutionary stability will be here in terms of families of utility functions. Of course, the cost of this approach is that results won't be explicit about the exact functional form that the ESS utility function will take.

\subsubsection{Locally stable utility functions}

The set $\Pf$ consists of all possible $2\times 2$ real matrices, which can be identified with the set $\Rn^{2\times 2}=\Rn^4$. The genotype of an individual can thus also be seen as the vector $\vpf = (\pf_{11},\pf_{12},\pf_{21},\pf_{22})$, where $\pf_{ij}$ denotes the utility to the focal player when he chooses action $i$ and his opponent chooses action $j$.

With such definitions, one can use standard approaches in evolutionary biology, such as adaptive dynamics, in order to find the ESS utility matrix, and the analysis can be complemented to look at convergence stability. Under this approach one considers a resident population with utility $\vpf$ and asks whether a mutant with utility $\vpf+\vd$ can invade or not. One advantage of this approach in our learning context is that the ``utility game'' between the resident and the mutant is quasi-symmetric, which implies that the learning dynamics determining behavior is a family of dynamics for quasi-symmetric games. This is a substantial simplification of the analysis. Save the bifurcations, the quasi-symmetric game between $\vpf$ and $\vpf+\vd$ is very close to the symmetric game defined by $\vpf$, because the vector field determining the learning dynamics is continuous in $\vpf$.

A candidate ESS is such that
\begin{equation}
\label{cES}
\left. \frac{\pd \fc}{\pd \vpf}\right|_{\vpf=\vpf^*} = 0 
\end{equation}

%More generally for a two-player symmetric game with $\na$ actions, there are $\na^2$ possible outcomes, hence there are $2^{\na^2}$ possible types of utility functions.








\subsection{Subset of strategies in the Prisoner's Dilemma}

Of the 7 strategies considered in Fig.~\ref{stratDiscrete}, the Realistic strategy (and the equivalent Avoid Sucker's) is the ``winning'' strategy (in a sense to be made precise later). Moreover it seems that among the set of Realistic strategies, those that put higher values on positive rewards would be favored, because they lead to faster learning (TO DO: check this). This sheds light on the maintenance of addiction to positive rewards in human populations. Overestimation of rewards (i.e.~give to a reward more value than its real fitness value) seems to be evolutionarily stable.

The strategy name ``Manipulator'' stems from the fact that an individual using this strategy will drive an indifferent opponent (i.e. that has positive utility for all four game outcomes) to cooperate, while the Manipulator against the indifferent opponent may converge to cooperation or defection depending on the stochastic events occurring during the game interaction.



\subsection{Analysis of the behavioral interactions between the strategies}

In this section we calculate the payoffs and fitnesses for all possible behavioral interactions between the 4 strategies Realistic, Other-regard, Selfish, and Manipulator. From these calculations, we derive invasion conditions within this set of strategies.

Some assumptions need to be made in order to compute analytical expressions for long-term payoffs. First, when the behavioral dynamics admit several stable equilibria, typically the stochastic dynamics may converge to any of these stable equilibria. It is very difficult to know which particular equilibrium will be reached by the stochastic process, because the trajectory may go out from the basin of attraction of a stable equilibrium under the influence of large stochastic shocks (these large shocks are more likely to occur at the beginning of the behavioral interaction). For these reasons, we assume that the initial preferences of players are unbiased, such that the process starts in the center of the state space ($\p_1 = \p_2 = 1/2$). Also, we assume that the process may reach each equilibrium with equal probability, in other words the distribution of stable equilibria is uniform. Such an assumption is valid for values of the exploration rate around $\xr\approx 10$ (Dridi \& Lehmann, 2015).


\begin{table}[]
\small
\caption{Classification of behavioral outcomes in the discrete action model amongst the 4 strategies considered.}
\label{behOut}
\rowcolors{1}{gray!25}{white}
\begin{tabular}{@{}llll@{}}
\toprule
    \textbf{Interaction}    & \textbf{Always stable equilibria}                       & \textbf{Sometimes stable equilibria}                                               & \textbf{Stability condition}                           \\ \midrule
R vs.~R & $(1,1)$ and $(0,0)$                                               &                                                                           &                                               \\ \midrule
R vs.~O & $(0,1)$ and $(1,1)$                            &                                                                           &                                               \\ \midrule
R vs.~M & $(1,1)$ and $(0,\frac{v_{22}}{v_{12}+v_{22}})$ &                                                                           &                                               \\ \midrule
R vs.~S & $(0,0)$                                        &                                                                           &                                               \\ \midrule
O vs.~O & $(1,1)$                                        &                                                                           &                                               \\ \midrule
O vs.~M & $(1,0)$ and $(1,1)$                            &                                                                           &                                               \\ \midrule
O vs.~S & $(1,0)$                                        &                                                                           &                                               \\ \midrule
M vs.~M & $(1,1)$                                        & $(\frac{u_{22}}{u_{12}+u_{22}},0)$ and $(0,\frac{v_{22}}{v_{12}+v_{22}})$ & $|u_{12}| < |u_{21}|$, $|v_{12}| < |v_{21}|$, \\ \midrule
M vs.~S & $(\frac{u_{22}}{u_{12}+u_{22}},0)$             &                                                                           &                                               \\ \midrule
S vs.~S & $(0,0)$                                        &                                                                           &                                               \\ \bottomrule
\end{tabular}
\end{table}





\begin{table}[]
\small
\caption{Generic total payoff in the discrete action model amongst the 4 strategies considered.}
\label{payOut}
\rowcolors{1}{gray!25}{white}
\begin{tabular}{@{}ll@{}}
\toprule
 \textbf{Interaction}       & \textbf{Generic fitness}                                                                                                                                                                                                                                                 \\ \midrule
R vs.~R & $(\frac{1}{2}(\Rp+\Pp), \frac{1}{2}(\Rp+\Pp))$                                                                                                                                                                                                                                                                \\ \midrule
R vs.~O & $(\frac{1}{2}(\Rp+\Tp), \frac{1}{2}(\Rp+\Sp))$                                                                                                                                                                                                                  \\ \midrule
R vs.~M & $\left(\frac{1}{2}\left(\Rp+\Tp\left(\frac{v_{22}}{v_{12}+v_{22}}\right)+\Pp\left(1-\frac{v_{22}}{v_{12}+v_{22}}\right)\right), \frac{1}{2}\left(\Rp+\Sp\left(\frac{v_{22}}{v_{12}+v_{22}}\right)+\Pp\left(1-\frac{v_{22}}{v_{12}+v_{22}}\right)\right)\right)$ \\ \midrule
R vs.~S & $(\Pp, \Pp)$                                                                                                                                                                                                                                                    \\ \midrule
O vs.~O & $(\Rp, \Rp)$                                                                                                                                                                                                                                                    \\ \midrule
O vs.~M & $(\frac{1}{2}(\Rp+\Sp),\frac{1}{2}(\Rp+\Tp),)$                                                                                                                                                                                                                  \\ \midrule
O vs.~S & $(\Sp, \Tp)$                                                                                                                                                                                                                                                    \\ \midrule
M vs.~M & $(\Rp,\Rp)$ or ...                                                                                                                                                                                                                                              \\ \midrule
M vs.~S & $\left(\Sp\left(\frac{u_{22}}{u_{12}+u_{22}}\right)+\Pp\left(1-\frac{u_{22}}{u_{12}+u_{22}}\right),\Tp\left(\frac{u_{22}}{u_{12}+u_{22}}\right)+\Pp\left(1-\frac{u_{22}}{u_{12}+u_{22}}\right)\right)$                                                          \\ \midrule
S vs.~S & $(\Pp, \Pp)$                                                                                                                                                                                                                                                    \\ \bottomrule
\end{tabular}
\end{table}


\bigskip


\begin{table}[]
\small
\caption{Total payoff in the 3 different types of games in the discrete action model amongst the 4 strategies considered.}
\label{payOut}
\rowcolors{1}{gray!25}{white}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Interaction} & \textbf{Prisoner's Dilemma}                                                                                           & \textbf{Stag Hunt}                                                                                                       & \textbf{Snowdrift}                                                                                                       \\ \midrule
R vs.~R              & $(\frac{\bn - \cs}{2}, \frac{\bn - \cs}{2})$                                                                                     & $(\frac{(\vh+1)(\bn - \cs)}{2\vh}, \frac{(\vh+1)(\bn - \cs)}{2\vh})$                                                                & $(\frac{2\bn - \cs}{4}, \frac{2\bn - \cs}{4})$                                                                                      \\ \midrule
R vs.~O              & $(\bn -\frac{\cs}{2}, \frac{\bn - \cs}{2})$                                                                                      & $(\frac{(\vh+1)(\bn - \cs)}{2\vh},\frac{\bn - \cs}{2})$                                                                             & $(\bn -\frac{\cs}{4}, \bn -\frac{3\cs}{4})$                                                                                         \\ \midrule
R vs.~M              & $(\frac{(\bn - \cs)v_{12}+(2\bn - \cs)v_{22}}{2(v_{12}+v_{22})}, \frac{(\bn - \cs)v_{12}+(\bn - 2\cs)v_{22}}{2(v_{12}+v_{22})})$ & $(\frac{(\bn - \cs)\vh + (\bn - \cs)}{2\vh},\frac{(\bn - \cs + (\bn - \cs)\vh)v_{12}+(\bn - 2\cs)\vh v_{22}}{2\vh(v_{12}+v_{22})})$ & $(\frac{(2\bn - \cs)v_{12}+(4\bn - \cs)v_{22}}{4(v_{12}+v_{22})}, \frac{(2\bn - \cs)v_{12}+(4\bn - 3\cs)v_{22}}{4(v_{12}+v_{22})})$ \\ \midrule
R vs.~S              & $(0,0)$                                                                                                                          & $(\frac{\bn - \cs}{\vh}, \frac{\bn - \cs}{\vh})$                                                                                    & $(0,0)$                                                                                                                             \\ \midrule
O vs.~O              & $(\bn -\cs, \bn -\cs)$                                                                                                           & $(\bn -\cs, \bn -\cs)$                                                                                                              & $(\bn -\frac{\cs}{2}, \bn - \frac{\cs}{2})$                                                                                         \\ \midrule
O vs.~M              & $(\frac{\bn}{2} - \cs, \bn - \frac{\cs}{2})$                                                                                     & $(\frac{\bn}{2} - \cs,\frac{(\bn - \cs)\vh+\bn - \cs}{2\vh})$                                                                       & $(\bn -\frac{3\cs}{4},\bn -\frac{\cs}{4})$                                                                                          \\ \midrule
O vs.~S              & $(-\cs, \bn)$                                                                                                                    & $(- \cs, \frac{\bn - \cs}{\vh})$                                                                                                    & $(\bn -\cs, \bn)$                                                                                                                   \\ \midrule
M vs.~M              & $(\bn -\cs, \bn -\cs)$                                                                                                           & $(\bn -\cs, \bn -\cs)$                                                                                                              & $(\bn -\frac{\cs}{2}, \bn - \frac{\cs}{2})$                                                                                         \\ \midrule
M vs.~S              & $(\frac{-\cs u_{22}}{u_{12}+u_{22}},\frac{\bn u_{22}}{u_{12}+u_{22}})$                                                           & $(\frac{(\bn - \cs)u_{12} - \cs\vh u_{22}}{\vh(u_{12}+u_{22})},\frac{\bn - \cs}{\vh})$                                              & $(\frac{(\bn-\cs) u_{22}}{u_{12}+u_{22}},\frac{\bn u_{22}}{u_{12}+u_{22}})$                                                         \\ \midrule
S vs.~S              & $(0,0)$                                                                                                                          & $(\frac{\bn - \cs}{\vh}, \frac{\bn - \cs}{\vh})$                                                                                    & $(0,0)$                                                                                                                             \\ \bottomrule
\end{tabular}
\end{table}




%\clearpage









%\begin{table}[]
%\tiny
%\caption{Classification of behavioral outcomes and resulting fitness in the discrete action model amongst the 4 strategies considered.}
%\label{behOut}
%\begin{sideways}
%\rowcolors{2}{gray!25}{white}
%\begin{tabular}{@{}llllllll@{}}
%\toprule
%        & Always stable equilibria                       & Sometimes stable equilibria                                               & Stability condition                           & Generic fitness                                                                                                                                                                                                                                                 & Fitness in PD & Fitness in SH & Fitness in SD \\ \midrule
%R vs.~R &                                                &                                                                           &                                               &                                                                                                                                                                                                                                                                 &               &               &               \\ \midrule
%R vs.~A & $(0,0)$ and $(1,1)$                            &                                                                           &                                               & $(\frac{1}{2}\Rp, \frac{1}{2}\Rp)$                                                                                                                                                                                                                              &               &               &               \\ \midrule
%R vs.~P & $(1,1)$ and $(0,\frac{v_{22}}{v_{12}+v_{22}})$ &                                                                           &                                               & $(\frac{1}{2}\Rp, \frac{1}{2}\Rp)$                                                                                                                                                                                                                              &               &               &               \\ \midrule
%R vs.~O & $(0,1)$ and $(1,1)$                            &                                                                           &                                               & $(\frac{1}{2}(\Rp+\Tp), \frac{1}{2}(\Rp+\Sp))$                                                                                                                                                                                                                  &               &               &               \\ \midrule
%R vs.~M & $(1,1)$ and $(0,\frac{v_{22}}{v_{12}+v_{22}})$ &                                                                           &                                               & $\left(\frac{1}{2}\left(\Rp+\Tp\left(\frac{v_{22}}{v_{12}+v_{22}}\right)+\Pp\left(1-\frac{v_{22}}{v_{12}+v_{22}}\right)\right), \frac{1}{2}\left(\Rp+\Sp\left(\frac{v_{22}}{v_{12}+v_{22}}\right)+\Pp\left(1-\frac{v_{22}}{v_{12}+v_{22}}\right)\right)\right)$ &               &               &               \\ \midrule
%R vs.~S & $(0,0)$                                        &                                                                           &                                               & $(\Pp, \Pp)$                                                                                                                                                                                                                                                    &               &               &               \\ \midrule
%O vs.~O & $(1,1)$                                        &                                                                           &                                               & $(\Rp, \Rp)$                                                                                                                                                                                                                                                    &               &               &               \\ \midrule
%O vs.~M & $(1,0)$ and $(1,1)$                            &                                                                           &                                               & $(\frac{1}{2}(\Rp+\Sp),\frac{1}{2}(\Rp+\Tp),)$                                                                                                                                                                                                                  &               &               &               \\ \midrule
%O vs.~S & $(1,0)$                                        &                                                                           &                                               & $(\Sp, \Tp)$                                                                                                                                                                                                                                                    &               &               &               \\ \midrule
%M vs.~M & $(1,1)$                                        & $(\frac{u_{22}}{u_{12}+u_{22}},0)$ and $(0,\frac{v_{22}}{v_{12}+v_{22}})$ & $|u_{12}| < |u_{21}|$, $|v_{12}| < |v_{21}|$, & $(\Rp,\Rp)$ or ...                                                                                                                                                                                                                                              &               &               &               \\ \midrule
%M vs.~S & $(\frac{u_{22}}{u_{12}+u_{22}},0)$             &                                                                           &                                               & $\left(\Sp\left(\frac{u_{22}}{u_{12}+u_{22}}\right)+\Pp\left(1-\frac{u_{22}}{u_{12}+u_{22}}\right),\Tp\left(\frac{u_{22}}{u_{12}+u_{22}}\right)+\Pp\left(1-\frac{u_{22}}{u_{12}+u_{22}}\right)\right)$                                                          &               &               &               \\ \midrule
%S vs.~S & $(0,0)$                                        &                                                                           &                                               & $(\Pp, \Pp)$                                                                                                                                                                                                                                                    &               &               &               \\ \bottomrule
%\end{tabular}
%\end{sideways}
%\end{table}


\clearpage




\begin{figure}[htb]
\begin{center}
\begin{tabular}{ccc}
& \begin{tabular}{|c|c|}
\hline
  \cellcolor{Turquoise}$C,C$ & \cellcolor{Turquoise}$C,D$ \\ \hline
\cellcolor{Turquoise}$D,C$ & \cellcolor{Turquoise}$D,D$\\ \hline
\end{tabular}
& \\
 & Outcome matrix & \\
 \\
 & \begin{tabular}{|c|c|}
\hline
  $\bn - \cs$ & $- \cs$ \\ \hline
$\bn$ & $0$ \\ \hline
\end{tabular}
& \\
 & Payoff matrix: $\bn>\cs>0$ & \\
 \\
& \begin{tabular}{|c|c|}
\hline
  \cellcolor{LimeGreen}$+$ & \cellcolor{red}$-$ \\ \hline
\cellcolor{LimeGreen}$+$ & \cellcolor{white}$0$\\ \hline
\end{tabular}
& \\
 & Realistic & \\
 \\
\begin{tabular}{|c|c|}
\hline
  \cellcolor{LimeGreen}$+$ & \cellcolor{red}$-$ \\ \hline
\cellcolor{red}$-$ & \cellcolor{red}$-$\\ \hline
\end{tabular}
 & \begin{tabular}{|c|c|}
\hline
  \cellcolor{LimeGreen}$+$ & \cellcolor{LimeGreen}$+$ \\ \hline
\cellcolor{red}$-$ & \cellcolor{red}$-$\\ \hline
\end{tabular}
 & \begin{tabular}{|c|c|}
\hline
  \cellcolor{LimeGreen}$+$ & \cellcolor{red}$-$ \\ \hline
\cellcolor{LimeGreen}$+$ & \cellcolor{LimeGreen}$+$\\ \hline
\end{tabular}
\\
Pareto & Other-regard & Avoid Sucker's payoff\\
\\ 
\begin{tabular}{|c|c|}
\hline
  \cellcolor{red}$-$ & \cellcolor{red}$-$ \\ \hline
\cellcolor{red}$-$ & \cellcolor{LimeGreen}$+$\\ \hline
\end{tabular}
 & \begin{tabular}{|c|c|}
\hline
  \cellcolor{red}$-$ & \cellcolor{red}$-$ \\ \hline
\cellcolor{LimeGreen}$+$ & \cellcolor{LimeGreen}$+$\\ \hline
\end{tabular}
 & \begin{tabular}{|c|c|}
\hline
  \cellcolor{LimeGreen}$+$ & \cellcolor{red}$-$ \\ \hline
\cellcolor{LimeGreen}$+$ & \cellcolor{red}$-$\\ \hline
\end{tabular}\\
Nash & Selfish & Manipulator\\ 
\end{tabular}
\caption{The 7 strategies considered in the discrete action model. A strategy is defined by the outcomes to which it associates a positive or negative utility in the corresponding outcome matrix (top). Cooperation is denoted by $C$ and defection by $D$. In the outcome matrix, the first letter refers to the action of the focal player (row) and the second letter to the action of its opponent (column). For example, the strategy Realistic associates a positive utility to the outcomes that yield positive material payoffs, and negative utility to outcomes yielding negative material payoffs. The strategy Pareto associates a positive utility to the outcome where both players cooperate ($C,C$) and has a negative utility for all other outcomes.}
\label{stratDiscrete}
\end{center}
\end{figure}




\begin{figure}
%\vspace*{.05in}
\begin{center}
\includegraphics[scale=0.6]{DSps.pdf}
\caption{Vector field (gray arrows) and stochastic trajectory (blue line) for the interaction between ``Pareto'' and ``Selfish''. On the $x$-axis is represented the probability that Pareto cooperates ($p_1$), while on the $y$-axis, this is the probability that Selfish cooperates ($p_2$). The stochastic trajectory is started from the center of the state space $(p_1,p_2) = (\frac{1}{2},\frac{1}{2})$ and dots on it represent interaction rounds between the players. Circles represent equilibria: a white-filled circle is a source (both associated eigenvalues are positive); a gray-filled circle is a saddle (one positive and one negative associated eigenvalue); a black circle is a sink (both associated eigenvalues are negative).}
\label{cS}
\end{center}
\end{figure}


\begin{figure}
%\vspace*{.05in}
\begin{center}
\includegraphics[scale=0.6]{DSpr.pdf}
\caption{Same as Fig.~\ref{cS} but for the interaction between Pareto and Realistic. Here the deterministic mean field equation admits two locally stable equilibria. The two stochastic trajectories of different color correspond to simulation runs that respectively converge to one of the locally stable equilibria.}
\label{dspr}
\end{center}
\end{figure}


\begin{figure}
%\vspace*{.05in}
\begin{center}
\includegraphics[scale=0.6]{DSrr.pdf}
\caption{Same as Fig.~\ref{cS} but for the interaction between Realistic and Realistic. The red-filled dot denotes an equilibrium with 0 eigenvalues.}
\label{dspr}
\end{center}
\end{figure}



%\begin{figure}[htb]
%\begin{center}
%\begin{game}{2}{2}
%& & \\
%& {$+$} &$-$\\
%&$-$&$-$\\
%\end{game}
%
%Test
%\caption[]{A game with a colored cell.}\label{f:coloredCell}
%\end{center}
%\end{figure}

%\subsection{Constant public-goods games}
%
%If we let $\om$ be the total benefit produced in a public-goods game, then we may look for utility functions defined over this outcome.
%
%\subsection{Nash demand game}
%








\clearpage


\section{Continuous action space}

\subsection{Normally distributed actions}

Here, we treat the case where the action space is one-dimensional continuous, e.g., when $\Ac_i \subseteq \Rn$ for all $i$. This allows to capture investment games such as a continuous public goods game. We  adapt a model of Beigy \& Meibody (2006), who study learning of a single decision-maker who faces a stochastic stationary environment. In their setting, the decision-maker tries to minimize the expectation of a given cost function where at each discrete time step he can exert an action $\ac \in \Rn$. In order to adapt their model to multi-player games, only a few tweaks need to be made.

To start with, we consider that at each time step $\tm$, every player is characterized by a mean action $\ma_{i,\tm}$, and a standard deviation $\sd_{i,\tm}$. The action $\ac_{i,\tm}$ chosen by $i$ at time $\tm$ is then drawn from a normal distribution with mean $\ma_{i,\tm}$ and standard deviation $\sd_{i,\tm}$, i.e., $\ac_{i,\tm} \sim \Nd(\ma_{i,\tm},\sd_{i,\tm})$. A possible interpretation of this model of action choice is that $\ma_{i,\tm}$ represents the preferred action by player $i$, which he would like to implement but there is a stochastic perturbation during action choice, such that $\ac_{i,\tm} = \ma_{i,\tm} + \varepsilon_{i,\tm}$, where $\varepsilon_{i,\tm} \sim \Nd(0,\sd_{i,\tm})$ is independent across players and time steps. With this interpretation, $\varepsilon_{i,\tm}$ can be seen as an exogenous noise during action implementation, or can also be seen as explicit exploration of the player around its preferred action (note that this model of action perturbation is different from the above model of payoff perturbation that generates the logit choice rule, eq.~\ref{logitchoice}).

Since a normal distribution is fully specified by its mean and standard deviation, in order to know the action distribution of a player at time $\tm+1$, we only need to know how $\ma_{i,\tm+1}$ and $\sd_{i,\tm+1}$ are updated given the previous values at time $\tm$ and the outcome of the game. We will use the following updating rule for the mean action,
\begin{equation}
\label{upCa}
\ma_{i,\tm+1} = \ma_{i,\tm} + \lr_\tm \pf_{i}(\ac_{i,\tm},\va_{-i,\tm},\e_\tm) \left( \frac{\ac_{i,\tm}- \ma_{i,\tm}}{\sd_{i,\tm}} \right),
\end{equation}
which essentially entails that the individual wants his new mean action $\ma_{i,\tm+1}$ to come closer to the chosen action $\ac_{i,\tm}$ if the obtained utility $\pf_{i}(\ac_{i,\tm},\va_{-i,\tm},\e_\tm)$ is positive, while he wants his mean action to move away from $\ac_{i,\tm}$ if the obtained utility is negative; the magnitude of such movement is proportional to the actual magnitude of the obtained utility. Consequently, this is one of the simplest way of capturing in a continuous action setting the biological notion of approach towards rewards and avoidance of punishments. %This thus implies that there is an increase in the mean action if the obtained utility $\pf_{i}(\ac_{i,\tm},\va_{-i,\tm},\e_\tm)$ and the difference $\ac_{i,\tm}- \ma_{i,\tm}$ have the same sign. Conversely, there is a decrease in the mean action if the obtained utility $\pf_{i}(\ac_{i,\tm},\va_{-i,\tm},\e_\tm)$ and the difference $\ac_{i,\tm}- \ma_{i,\tm}$ have opposite sign.
The standard deviation $\sd_{i,\tm}$ appears in the denominator of the reinforcement in eq.~\ref{upCa} so that the individual accounts for its own exploration strategy when updating action value: if $\sd_{i,\tm}$ is large, then there is a big chance that $\ac_{i,\tm}$ will be far from the mean, so the utility obtained by the individual should not be interpreted as reflecting a wrong mean action $\ma_{i,\tm}$ (the utility is mainly a reflection of the great variance in action choice), hence the update should be minor. Finally, the learning rate $\lr_\tm$ must obey the assumptions of stochastic approximation theory, just as in the discrete-action model (eq.~\ref{Lrule2}) in order to ensure convergence of the learning process.

In the case where $\sd_{i,\tm} = \sd_{i}$ is a constant, independent of time, then  $\sd_{i}$ acts as an additional learning rate, that scales the speed at which the mean-field deterministic dynamics converge to an equilibrium: larger variance should lead to faster convergence (see the analysis below for details).

A technical assumption needed to make the analysis tractable is that $\pf_{i}(\ac_{i,\tm},\va_{-i,\tm},\e_\tm)$ is twice continuously differentiable in the actions of all the players (note that this may create some problems when defining games with bounded action space, where typically the payoff function is not differentiable on the boundaries).

The vector $\vma_\tm = (\ma_{1,\tm},\dots,\ma_{\np,\tm})$ collects the mean action of all players at time $\tm$.


\subsection{Stochastic approximation}

Let $\rf_{i,t+1} = \pf_{i}(\ac_{i,\tm},\va_{-i,\tm},\e_\tm) \left( \frac{\ac_{i,\tm}- \ma_{i,\tm}}{\sd_{i,\tm}} \right)$ be the reinforcement to the mean action of player $i$, and let $\vrf_{\tm+1} = (\rf_{i,\tm+1})_{i\in\np}$ denote the vector collecting the reinforcements of all the players at time $\tm+1$. Stochastic approximation theory tells us that $\Esp[\vrf_{t+1} | \vma_\tm]$ is a vector field that defines a differential equation for $\vma$ whose asymptotic path is followed by the original stochastic process $\{ \vma_\tm \}_{t\geq 0}$. To obtain an explicit expression for $\Esp[\vrf_{t+1} | \vma_\tm]$, we use a second-order Taylor series expansion of $\pf_{i}(\ac_{i,\tm},\va_{-i,\tm},\e_\tm)$ around $\ma_{i,\tm}$, which gives
\begin{multline}
\label{RfExp}
\Esp[\rf_{i,t+1} | \vma_\tm] =  \Esp \Big[ \Big(\pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm) + \nabla \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm)^{\textrm{T}} (\va_\tm  - \vma_\tm) \\
 + (\va_\tm  - \vma_\tm)^{\textrm{T}} \Hs \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm) (\va_\tm  - \vma_\tm) \Big) \frac{(\ac_{i,\tm}- \ma_{i,\tm})}{\sd_{i}} \Big| \vma_\tm \Big],
\end{multline}
where $\nabla \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm) = \left(\frac{\pd \pf_{i}}{\pd \ma_{1,\tm}},\dots,\frac{\pd \pf_{i}}{\pd \ma_{\np,\tm}}\right)$ is the gradient vector and $\Hs \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm)$ is the Hessian matrix of $\pf_{i}$, with typical element $[\Hs \pf_{i}]_{kj} = \frac{\pd^2 \pf_{i}}{\pd \ma_{k,\tm} \pd \ma_{j,\tm}}$. The vector $\va_\tm  - \vma_\tm$ has typical element $\ac_{j,\tm}- \ma_{j,\tm}$. Taking the non-random terms out of the expectation, the above equation can be rewritten as
\begin{multline}
\label{RfExp2}
\Esp[\rf_{i,t+1} | \vma_\tm] = \frac{1}{\sd_{i}} \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm) \Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) \Big| \vma_\tm \Big] + \nabla \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm) \Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) (\va_\tm  - \vma_\tm) \Big| \vma_\tm \Big] \\
+ \Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) (\va_\tm  - \vma_\tm)^{\textrm{T}} \Hs \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm) (\va_\tm  - \vma_\tm) \Big| \vma_\tm \Big].
\end{multline}
In order to find expressions for the various expectations above it will be useful to keep in mind that for each player $i$, the random variable $\ac_{i,\tm}- \ma_{i,\tm}=\varepsilon_{i,t}$ is normally distributed with zero mean and variance $\sd_{i}^2$. In particular, this implies that $\Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) \Big| \vma_\tm \Big] = 0$ for all $i$, so that $\Esp \Big[ (\va_\tm  - \vma_\tm) \Big| \vma_\tm \Big] = (0, 0, \dots,0)$. Also
\begin{align}
\label{RfExp3}
\Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) (\va_\tm  - \vma_\tm) \Big| \vma_\tm \Big] & = \Big( \cov[\ac_{1,\tm},\ac_{i,\tm} | \vma_\tm],\dots,\var[\ac_{i,\tm}| \vma_\tm] ,\dots, \cov[\ac_{\np,\tm},\ac_{i,\tm}| \vma_\tm] \Big) \nonumber\\ 
 & = \Big( 0,\dots,\sd_{i}^2,\dots, 0 \Big),
\end{align}
because action choice is independent across players. An interesting note here on social learning is that if players copy each other then $\cov[\ac_{j,\tm},\ac_{i,\tm}| \vma_\tm]$ of players $i$ and $j$ may be different than 0, depending on whether they can copy one another. The term involving the Hessian matrix has the form
\begin{multline}
\label{RfExpH}
\Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) (\va_\tm  - \vma_\tm)^{\textrm{T}} \Hs \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm) (\va_\tm  - \vma_\tm) \Big| \vma_\tm \Big] = \\
 \sum_{j\in\np} \sum_{k\in\np} \frac{\pd^2 \pf_{i}}{\pd \ma_{k,\tm} \pd \ma_{j,\tm}} \Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) (\ac_{j,\tm}- \ma_{j,\tm}) (\ac_{k,\tm}- \ma_{k,\tm}) \Big| \vma_\tm \Big],
\end{multline}
where the triplet covariance $\Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) (\ac_{j,\tm}- \ma_{j,\tm}) (\ac_{k,\tm}- \ma_{k,\tm}) \Big| \vma_\tm \Big] $ is zero again because action choice is independent across players whenever at least one of  $j\neq i$ or $k\neq i$ or $k\neq j$ is true. When $j=i$ and $k=i$, this triplet equals $\Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm})^3 \Big| \vma_\tm \Big]=0$. Thus we have
\begin{equation}
\label{RfExpH2}
\Esp \Big[ (\ac_{i,\tm}- \ma_{i,\tm}) (\va_\tm  - \vma_\tm)^{\textrm{T}} \Hs \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm) (\va_\tm  - \vma_\tm) \Big| \vma_\tm \Big] = 0.
\end{equation}
Plugging back the expressions of these expectations in eq.~\ref{RfExp2}, the expected reinforcement of player $i$ takes the simple form
\begin{equation}
\label{RfExp}
\Esp[\rf_{i,t+1} | \vma_\tm] = \sd_{i}  \frac{\pd \pf_{i}(\ma_{i,\tm},\vma_{-i,\tm},\e_\tm)}{\pd \ma_{i,\tm}}.
\end{equation}
Thus, the differential equation
\begin{equation}
\label{meanF}
\frac{\de \ma_i}{\de \tm} \equiv \dot{\ma}_i = \sd_{i}  \frac{\pd \pf_{i}(\ma_{i},\vma_{-i},\e)}{\pd \ma_{i}}, \qquad i\in\np
\end{equation}
describes the long-run stochastic dynamics of the mean action of player $i$. Eq.~\ref{meanF} is a gradient dynamical system, where every player can be seen as maximizing his own utility function. In previous work, such gradient dynamics have been used as an \emph{ad hoc} model of learning dynamics. It is interesting that we recover this result by explictly modeling the stochastic learning dynamics.

A counterintuitive feature of eq.~\ref{meanF}, when we compare it to eq.~\ref{upCa}, is that according to eq.~\ref{meanF}, learning speed should increase with the standard deviation $\sd_{i}$, while in the original stochastic updating rule (eq.~\ref{upCa}), larger $\sd_{i}$ should lead to smaller updates to the mean action $\ma_{i,\tm+1}$.

It is noteworthy that at an equilibrium for the means $\hat{\vma}$, action choice is still stochastic so that the equilibrium action of every player is $\hat{\ac}_i \sim \Nd(\hat{\ma}_{i},\sd_{i})$.


\subsection{Simulations}

TO DO: We will simulate the above game dynamics (eq.~\ref{upCa}) in continuous-payoff games in order to compare it to our analysis based on stochastic approximation.

\subsection{Bounded action space}

A difficulty that may arise from defining the chosen action as a normal random variable is that normal random variables are defined over the entire real line, $\Rn$. In certain cases, we may want to consider a bounded state space, of the form $(0,\mx)$. One way to achieve this is to make the transformation
\begin{equation}
\label{logitTr}
\ac_i = \frac{\mx}{1+ \exp[-\tilde{\ac}_i]},
\end{equation}
where $\tilde{\ac}_i \in \Rn$ is the variable that is dynamically updated in eq.~\ref{upCa}, and $\ac_i \in (0,\mx)$ is the action of individual $i$. For instance in a public-goods game, $\mx$ may represent the endowment of each player, which is the maximum contribution he can make to the public good.



\subsection{Baseline simple individual decision problem}

In this section we display simulations of the learning algorithm in the 1-player case under a deterministic environment. The utility function used here is given by
\begin{equation}
\label{cIndSim}
\pf_i(\ac_i) = - (\ac_i - \ba)^2,
\end{equation}
which displays a single global optimum action achieved at $\ac_i = \ba$. Though there is nothing particularly exciting about this model, it is used only to show numerically that our stochastic approximation results presented above do hold and help predict the real dynamics of the players' actions.

TO DO: test in multi-player games with more complex payoff functions.




\begin{figure}[h]
\begin{center}
\begin{tabular}{l}
  (A) \\
\includegraphics[scale=0.5]{simUf.pdf}\\
  (B) \\
 \includegraphics[scale=0.5]{meanDyn.pdf}\\
\end{tabular}
\caption{Continuous learning in a deterministic 1-player game. (A) The function $\pf_i(\ac_i)$ to be maximized by the player (eq.~\ref{cIndSim}), with $\ba=0.6$. (B) Typical dynamics of the mean action, $\ma_{i,\tm+1}$ when the utility function is that in (A).}
\label{simIndDyn}
\end{center}
\end{figure}



%\begin{figure}[h]
%%\vspace*{.05in}
%\begin{center}
%\caption{Stochastic trajectories in the convex set of feasible action probabilities.}
%\label{cS}
%\end{center}
%\end{figure}









\newpage

%\section{Figures}


%\begin{figure}[h]
%%\vspace*{.05in}
%\begin{center}
%\caption{Stochastic trajectories in the convex set of feasible action probabilities.}
%\label{cS}
%\end{center}
%\end{figure}



\end{document}